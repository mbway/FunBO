{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to automatically reload modules who's content has changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# configure matplotlib\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funbo as fb\n",
    "import funbo.plotting as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_bounds = [('x', 0, 10)]\n",
    "range_bounds = (-5, 5)\n",
    "\n",
    "class Coordinator(fb.Coordinator):\n",
    "    def get_pre_phase_config(self, trial_num):\n",
    "        c = fb.GPPriorSelectConfig(self.optimiser)\n",
    "        c.control_xs = fb.RegularGrid(100, self.optimiser.domain_bounds)\n",
    "        return c\n",
    "\n",
    "    def get_bayes_config(self, trial_num):\n",
    "        c = fb.BayesSelectConfig(self.optimiser)\n",
    "        c.surrogate = fb.GPySurrogate(\n",
    "            init_params=dict(\n",
    "                #NOTE: ARD makes a huge difference in this task\n",
    "                kernel = GPy.kern.RBF(input_dim=self.optimiser.surrogate_dimensionality(), ARD=True)\n",
    "            ),\n",
    "            optimise_params=dict(\n",
    "                parallel=False,\n",
    "                num_restarts=1\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        c.surrogate = fb.DistributedGPSurrogate(\n",
    "            init_params=dict(\n",
    "                num_experts=10,\n",
    "                kernel=sk_gp.kernels.ConstantKernel() * sk_gp.kernels.RBF() + sk_gp.kernels.WhiteKernel(),\n",
    "            ),\n",
    "            optimise_params=dict(\n",
    "                iterations=1,\n",
    "                randomize_theta=True,\n",
    "                extra_starts=2,\n",
    "            ),\n",
    "            predict_params=dict(\n",
    "                method='gPOE'\n",
    "            ),\n",
    "            parallel=True\n",
    "        )\n",
    "        '''\n",
    "        e = fb.WeightedExtractionConfig(self.optimiser)\n",
    "        e.tracking_l = 10\n",
    "        e.control_xs = fb.RegularGrid(20, self.optimiser.domain_bounds)\n",
    "        e.aux_optimiser_params = dict(\n",
    "            num_random=1_000,\n",
    "            # since extraction consists of many 'easy' 1D optimisations, BFGS is\n",
    "            # only needed to slightly tweak the best random result.\n",
    "            num_take_random=1,\n",
    "            num_bfgs=0,\n",
    "            exact_gradient=False,\n",
    "            quiet=True # don't show warnings\n",
    "        )\n",
    "        \n",
    "        c.extraction_config = e\n",
    "        return c\n",
    "\n",
    "\n",
    "def make_objective(to_fit, sample_num, sample_dist):\n",
    "    def objective(f):\n",
    "        _, xmin, xmax = domain_bounds[0]\n",
    "        # global reward\n",
    "        R_g = fp.utils.integrate(lambda x: (f(x) - to_fit(x))**2, (xmin, xmax))\n",
    "        # local rewards\n",
    "        R_ls = []\n",
    "        if sample_dist == 'linear':\n",
    "            reward_xs = np.linspace(xmin, xmax, num=sample_num)\n",
    "        elif sample_dist == 'random':\n",
    "            reward_xs = np.random.uniform(xmin, xmax, size=(sample_num,))\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        for x in reward_xs:\n",
    "            R_l = (f(x)-to_fit(x))**2\n",
    "            R_ls.append((x, R_l))\n",
    "        return R_ls, R_g\n",
    "    return objective\n",
    "\n",
    "# set this global after each optimisation\n",
    "op = None\n",
    "\n",
    "def test_approx_function(to_fit, coordinator, objective=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        to_fit: the function to approximate\n",
    "        trials: (pre_phase_trials, max_trials)\n",
    "    \"\"\"\n",
    "    if objective is None:\n",
    "        objective = make_objective(to_fit, sample_num=50, sample_dist='linear')\n",
    "\n",
    "    np.random.seed(0)\n",
    "    opt = fb.Optimiser(objective, domain_bounds, range_bounds, desired_extremum='min')\n",
    "    opt.run(coordinator)\n",
    "    fp.plot_convergence(opt, best_R_g=0)\n",
    "    fp.plot_trials(opt, opt.trials, to_fit, color_by_reward=True)\n",
    "    fp.plot_surrogate_with_trials(opt, -1, to_fit)\n",
    "    \n",
    "    inc_i, inc = opt.get_incumbent()\n",
    "    print('incumbent = trial {}'.format(inc_i))\n",
    "    fp.plot_trial_area(opt, inc, to_fit)\n",
    "    \n",
    "    global op\n",
    "    op = opt\n",
    "\n",
    "def plot_to_fit(to_fit):\n",
    "    _, xmin, xmax = domain_bounds[0]\n",
    "    xs = np.linspace(xmin, xmax, num=100)\n",
    "    plt.plot(xs, [to_fit(x) for x in xs], 'k--', label='to fit')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fit(x):\n",
    "    return np.sin(x) * 2*np.cos(x/4)\n",
    "plot_to_fit(to_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fitting a simple function with a single attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_approx_function(to_fit, Coordinator(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.plot_timings(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.plot_trial_quantities(op, {\n",
    "    'data set size': lambda t: t.selection.fitting.data_set_size if t.is_bayes() else 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op.trials[-1].selection.extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.plot_surrogate_3D(op, op.trials[-1].surrogate, flip_z=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fitting a simple function with a two attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_approx_function(to_fit, Coordinator(3, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.plot_surrogate_3D(op, op.trials[-1].surrogate, flip_z=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fitting with only a single random sample in the pre-phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = test_approx_function(to_fit,  Coordinator(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimise a noisy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_fit_noisy(x):\n",
    "    size = x.size if isinstance(x, np.ndarray) else None\n",
    "    return to_fit(x) + np.random.normal(loc=0, scale=0.2, size=size)\n",
    "\n",
    "plt.subplots(figsize=(16,8))\n",
    "plot_to_fit(to_fit_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_approx_function(to_fit_noisy, Coordinator(4, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_surrogate_3D(op, op.trials[-4].surrogate, flip_z=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a function which has flat regions and more intricate detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = WeightedBasisFunctions(arguments=[(2, 2, 1, 6), (6, 1, 1, 1), (8, 1, 2, 2)], # center, width, weight, power\n",
    "                               functions=Activations.super_Gaussian)\n",
    "def plot_super_gaussian():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    _, xmin, xmax = domain_bounds\n",
    "    xs = np.linspace(xmin, xmax, num=200)\n",
    "    f.plot(ax, xs, color='r', show_basis=False, label='Super Gaussian')\n",
    "plot_super_gaussian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_approx_function(f, Coordinator(10, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample R_l more frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, xmin, xmax = domain_bounds\n",
    "objective = make_objective(f, 20, 'linear')\n",
    "\n",
    "class Coordinator2(fb.Coordinator):\n",
    "    def get_pre_phase_config(self, trial_num):\n",
    "        c = fb.GPPriorSelectConfig(self.optimiser)\n",
    "        return c\n",
    "\n",
    "    def get_bayes_config(self, trial_num):\n",
    "        c = fb.BayesSelectConfig(self.optimiser)\n",
    "        c.surrogate.init_params['kernel'] = GPy.kern.RBF(input_dim=2, ARD=True)\n",
    "        c.tracking_l = 10\n",
    "        return c\n",
    "\n",
    "test_approx_function(f, Coordinator2(5, 10), objective=objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_surrogate_3D(op, op.trials[-4].surrogate, flip_z=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling even more frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, xmin, xmax = domain_bounds\n",
    "objective = make_objective(f, 40, 'linear')\n",
    "\n",
    "class CoordinatorSparse(fb.Coordinator):\n",
    "    def get_pre_phase_config(self, trial_num):\n",
    "        c = fb.GPPriorSelectConfig(self.optimiser)\n",
    "        return c\n",
    "\n",
    "    def get_bayes_config(self, trial_num):\n",
    "        c = fb.BayesSelectConfig(self.optimiser)\n",
    "        c.surrogate = GPySurrogate(\n",
    "            init_params=dict(\n",
    "                kernel = GPy.kern.RBF(input_dim=2),\n",
    "                num_inducing=20,\n",
    "                normalizer=True\n",
    "            ),\n",
    "            optimise_params=dict(\n",
    "                parallel = False, # Can't use parallel optimisation with sparse GP (bug)\n",
    "                verbose = True,\n",
    "                num_restarts = 1\n",
    "            ),\n",
    "            sparse=True\n",
    "        )\n",
    "        c.tracking_l = 10\n",
    "        return c\n",
    "\n",
    "test_approx_function(f, CoordinatorSparse(10, 20), objective=objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the effect of sampling R_l at different places each trial\n",
    "Also in the pre_phase the sampled functions are engineered to hopefully be more representative of the function to fit (non-zero mean and variance > 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, xmin, xmax = domain_bounds\n",
    "objective = make_objective(f, 10, 'random')\n",
    "\n",
    "class Coordinator3(fb.Coordinator):\n",
    "    def get_pre_phase_config(self, trial_num):\n",
    "        c = fb.GPPriorSelectConfig(self.domain_bounds)\n",
    "        c.mu = lambda x: 1.0 # bias\n",
    "        c.kernel = GPy.kern.RBF(input_dim=1, variance=1.5, lengthscale=1.0) # TODO: not multi-dimensional\n",
    "        return c\n",
    "\n",
    "    def get_bayes_config(self, trial_num):\n",
    "        c = fb.BayesSelectConfig(self.domain_bounds)\n",
    "        return c\n",
    "\n",
    "test_approx_function(f, Coordinator3(10, 15), objective=objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_surrogate_3D(op, op.trials[-4].surrogate, flip_z=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
