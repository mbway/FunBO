{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to automatically reload modules who's content has changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# configure matplotlib\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import GPy\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import function_bo as fbo\n",
    "from function_bo_plotting import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../spring_task')\n",
    "from pygame_spring import Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = Simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(states):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    ts = [s.t for s in states]\n",
    "    \n",
    "    ax1.set_title('positions')\n",
    "    ax1.plot(ts, [s.target.y for s in states], label='target y')\n",
    "    ax1.plot(ts, [s.ee_pos.y for s in states], label='EE y')\n",
    "    ax1.set_ylabel('y position')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.set_title('recorded gains')\n",
    "    ax2.plot(ts, [s.Kp for s in states], label='Kp')\n",
    "    ax2.plot(ts, [s.Kd for s in states], label='Kp')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.legend()\n",
    "    \n",
    "    ax3.set_title('position error')\n",
    "    ax3.plot(ts, [s.target.y-s.ee_pos.y for s in states], label='error')\n",
    "    ax3.set_xlabel('time')\n",
    "    ax3.set_ylabel('error')\n",
    "    ax3.legend()\n",
    "    \n",
    "    ax4.set_title('force')\n",
    "    ax4.plot(ts, [s.spring_f.y for s in states], label='real spring force y')\n",
    "    ax4.set_xlabel('time')\n",
    "    ax4.set_ylabel('y force')\n",
    "    ax4.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "def trajectory(t):\n",
    "    if t < 6:\n",
    "        return (0, 100 + 50*t)\n",
    "    else:\n",
    "        return None # finish\n",
    "    \n",
    "f_max = 600\n",
    "Kp_max = 6\n",
    "\n",
    "def objective(f, fps=2000):\n",
    "    def normalize_spring_f(spring_f_y):\n",
    "        return np.abs(spring_f_y)/f_max\n",
    "        \n",
    "    def gains(spring_f):\n",
    "        Kp_y = f(normalize_spring_f(spring_f.y)) * Kp_max\n",
    "        return (Kp_y, 0)\n",
    "    states = sim.run(fps=fps, virtual_spring_gains=gains, real_spring_gains=(1.5, 0.2), ee_initial_pos=(0, 50), trajectory=trajectory)\n",
    "    R_ls = []\n",
    "    R_g = 0\n",
    "    eval_info = {'states':states}\n",
    "    \n",
    "    n = len(states)\n",
    "    penalty = []\n",
    "    closeness = []\n",
    "    \n",
    "    for i, s in enumerate(states):\n",
    "        reaction_f = -1 * s.spring_f\n",
    "        penalty.append(np.linalg.norm(np.multiply(reaction_f, s.ee_vel)))\n",
    "        closeness.append(np.linalg.norm(s.target-s.ee_pos))\n",
    "    \n",
    "    # np.diff takes the differences between adjacent elements\n",
    "    force_reward = 0.5*sigmoid(np.array(penalty)) + 0.5*sigmoid(np.hstack([np.diff(penalty), 0]))\n",
    "    goal_reward = 1.5*sigmoid(np.array(closeness))\n",
    "    reward = -1 * force_reward + goal_reward\n",
    "    \n",
    "    #sample_at = np.random.permutation(np.arange(len(states)))[:15]\n",
    "    sample_at = np.arange(len(states))\n",
    "    R_ls = [(normalize_spring_f(states[i].spring_f.y), reward[i]) for i in sample_at]\n",
    "    R_g = np.sum(reward)\n",
    "\n",
    "    return R_ls, R_g, eval_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_constant_gains():\n",
    "    R_ls, R_g, eval_info = objective(lambda f: 1)\n",
    "    s = eval_info['states']\n",
    "    print(R_g)\n",
    "    print(R_ls)\n",
    "    plot_summary(s)\n",
    "test_constant_gains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_bounds = ('f', 0, 1) # spring force\n",
    "range_bounds = (0.1, 1)   # stiffness\n",
    "\n",
    "class Coordinator(fbo.Coordinator):\n",
    "    def get_pre_phase_config(self, trial_num):\n",
    "        c = fbo.GPPriorSelectConfig(self.domain_bounds)\n",
    "        c.mu = lambda x: 2.5 + x\n",
    "        #c.kernel = GPy.kern.Bias(input_dim=1, variance=0.2) + GPy.kern.Linear(input_dim=1, variances=(1,))\n",
    "        c.kernel = GPy.kern.RBF(input_dim=1, variance=2, lengthscale=1)\n",
    "        return c\n",
    "\n",
    "    def get_bayes_config(self, trial_num):\n",
    "        c = fbo.BayesSelectConfig(self.domain_bounds)\n",
    "        c.kernel = GPy.kern.RBF(input_dim=2, ARD=False)\n",
    "        #c.sparse_GP = True\n",
    "        #c.surrogate_optimise_iterations = 2\n",
    "        #c.surrogate_optimise_parallel = False # can't with sparse GP\n",
    "        #c.surrogate_optimise_verbose = True\n",
    "        #c.tracking_l = 0.4\n",
    "        return c\n",
    "    \n",
    "coordinator = Coordinator(domain_bounds, 10, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "op = fbo.Optimiser(objective, domain_bounds, range_bounds, desired_extremum='min', coordinator=coordinator)\n",
    "op.run()\n",
    "plot_convergence(op, best_R_g=None)\n",
    "plot_trials(op, op.trials, color_by_reward=True)\n",
    "plot_surrogate_with_trials(op, -1)\n",
    "\n",
    "inc_i, inc = op.get_incumbent()\n",
    "print('incumbent = trial {}'.format(inc_i))\n",
    "plot_trials(op, [inc], color_by_reward=True)\n",
    "#plot_trial_area(op, inc, to_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_surrogate_3D(op, op.trials[-1].surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_incumbent():\n",
    "    R_ls, R_g, eval_info = objective(inc.f, fps=20)\n",
    "    s = eval_info['states']\n",
    "    print(R_g)\n",
    "    print(R_ls)\n",
    "    plot_summary(s)\n",
    "test_incumbent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
